Chapter 4 신경망 학습
    학습: 훈련 데이터로 부터 가중치 매개변수 최적값을 자동으로 찾는 것
            -> 손실 함수의 결과값을 가장 작게 만드는 매개변수 찾는 것을 목표

손실 함수: 신경망 학습에서 사용하는, 신경망의 나쁨을 나타내는 지표
            -> 일반적으로 평균제곱오차, 교차 엔트로피 오차 사용

평균제곱오차
    E = (1/2)sum(yk-tk)^2
    y: 신경망의 출력 [0.1,0.05,0.6,0.0 ...]
    t: 정답레이블    [0,0,1,0,.....]
        -> 정답을 가리키는 위치: 1, 그 외: 0
    
    원-핫 인코딩: 정답레이블에서 한 원소만 1, 나머지는 0

교차 엔트로피 오차
    E = -(sum(tk log yk) (log는 자연로그)
        주의) 구현할때, log()의 입력이 0이 되지 않도록 할 것
            (log0 = -무한대로 발산)

미니배치 학습
    모든 훈련데이터에 대한 손실 함수의 합을 구하는 방법
    수백~수천만개의 훈련 데이터 중 학습을 위해 골라낸 데이터셋의 일부

손실함수를 설정하는 이유
    신경망 학습 - 손실 함수의 결과값을 가장 작게 만드는 매개변수를 찾음
        매개변수의 미분(기울기)을 계산 -> 매개변수 값을 서서히 갱신
        매개변수 변경시 함수 값이 연속적으로 변함

    궁극적인 목적: 정확도
    but, 정확도의 미분값은 대부분의 장소에서 0
        매개변수 변경시 변화 없거나, 불연속적으로 변화함
        -> 매개변수 갱신 불가능
        비슷한 이유로 계단함수 또한 손실함수로 사용하지 않음

    결론: 기울기가 0이 되지 않아야만, 신경망이 올바르게 학습할 수 있음

미분

해석적미분
    - 수식을 전개해 미분
        ex) y=x^2 -> y=2x

        오차가 없는 진정한 미분 값 구할 수 있음

수치미분
    - 아주 작은 차분으로 미분

    미분: 접선의 기울기 -> x의 작은 변화가 f(x)를 얼마 만큼 변화시키는가...
    

    구현: h=10e-50 
        문제사항
            1. 반올림 오차 발생 
                -> h=10e-4로 변경
            2. h를 무한히 0 으로 만들 수 없음 -> 오차 발생
                -> (x-h), (x+h)일때의 차분을 계산하는 방법으로 오차 줄임
                (x를 중심으로 전후의 차분 계산: 중심차분(=중앙차분))

편미분
    -변수가 여럿인 함수에 대한 미분

    구현 1.
        1. x0, x1중 하나를 고정
        2. 나머지 변수로만 이루어진 함수 생성
        3. 수치미분
    
    구현 2.
        기울기 계산

기울기
    - 모든 변수의 편미분을 벡터로 정리한 것
    기울기가 가리키는 방향: 각 장소에서 함수의 출력값을 가장 크게 줄이는 방향

    경사법
        - 기울기를 이용해 함수의 최솟값을 찾는 방법
        기울어진 방향으로 일정 거리 이동 -> 해당 지점에서 기울기 계산 -> 이동 -> ...

        최솟값을 찾는 경사법 - 경사하강법
        최댓값을 찾는 경사법 - 경사상승법

        주의) 함수값을 낮추는 방안을 제시하는 지표: 기울기
            but, 정말 최솟값이 있는지, 방향이 맞는지 보장 불가능함
            하지만 해당 방향으로 이동시, 함수의 값을 줄일수는 있음
                => 최솟값의 장소를 찾는 문제에서 나아갈 방향의 단서로 사용

            극소값, 극댓값, 안장점에서 기울기 = 0
                    -> 기울기=0 이 최소값이라고 보장 할 수 없음
        
        학습률
            - 한 번의 학습으로 얼마만큼 학습할지(매개변수 값을 갱신 할지) 결정하는 상수
            너무 크거나 작으면 '좋은 장소'를 찾을 수 없음
        
        신경망에서의 기울기
            - 가중치 매개변수에 대한 손실함수의 기울기
            L: 손실함수, W: 가중치 일때
                편미분의 형상이 W와 같음

    하이퍼 파라메터 - 사람이 직접 설정해야 하는 매개변수 (ex: 학습률)

학습 알고리즘 구현하기

    확률적 경사 하강법(Stochastic gradient descent)
        - 확률적으로 무작위로 골라낸 데이터에 대해 수행하는 경사 하강법
    전제: 적응가능한 가중치, 편향 존재
            학습: 파라메터(가중치, 편향)를 훈렬 데이터에 적응하도록 조정하는 과정
    1. - 미니배치
        훈련 데이터 중 일부를 무작위로 가져옴 <= 미니배치
        목표: 미니배치의 손실 함수값 최소화
    2. - 기울기 산출
        목표달성을 위해 각 가중치 매개변수의 기울기 구함
        이때, 기울기는 목표 달성을 위한 방향 제시
    3. - 매개변수 갱신
        가중치 매개변수를 기울기 방향으로 아주 조금 갱신
    4. - 반복

시험 데이터로 평가하기
    에폭(epoch) - 학습에서 훈련데이터를 모두 소진했을때의 횟수를 나타내는 단위
    ex) 훈련데이터 10000개를 100개의 미니배치로 학습 할 경우 -> 100회 반복
            -> 1에폭 = 100회

numpy 메소드
np.random.choice(60000,10) 0~59999(60000 미만)의 숫자 중 10개 추출