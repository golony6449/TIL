Chapter 5 오차역전파법

오차역전파법을 이용한 신경망 구현
- 학습알고리즘과 유사
but, 계층을 사용해, 수치미분과정 없음
계층을 올바른 순서로 연결한 뒤, 호출해서 사용

그러면 수치미분은 필요없는 건가?
-> 검증에 사용

오차역전파법으로 구한 기울기 검증하기
수치미분 - 느림, 구현간단, 실수요소적음
오차역전파법(해석적 미분) - 빠름, 구현복잡, 실수요소 많음
-> 오차역전파법의 검증 목적으로 수치미분 사용

Chapter 6 학습 관련 기술

매개변수 갱신
최적화: 손실함수의 값을 가능 한 낮추는 매개변수를 찾는 것

확률적 경사 하강법(SGD)
매개변수의 기울기를 구해, 기울어진 방향으로 매개변수 갱신 반복
장점: 간단한 구현
단점: 비등방성 함수에서의 비효율성
비등방성 함수: 방향에 따라 성질(기울기)이 달라지는 함수
비등방성 함수의 예: (1/20)*x**2 + y**2

단점 개선법: 모멘텀, AdaGrad, Adam

모멘텀(Momentum)
변화율을 물리의 속도 처럼 간주
수식
v <- av -학습률 * 손실함수 기울기
W < W + v

AdaGrad
학습률 감소 방법(학습을 진행 하면서 학습률을 줄여나감)
학습률의 감소가 매개변수의 원소 마다 다르게 적용
(크게 갱신된 원소 -> 학습률 또한 크게 감소)

결과: 갱신량이 큰 y축의 학습률 감소 -> y축으로는 변화 거의 없음

Adam
모멘텀 + AdaGard

일반적으로 SGD 보다 나머지 3가지 기법의 학습속도가 더 빠름 
(때에 따라서는 최종정확도 또한 높음)

가중치의 초기값


python
collections 모듈 - 여러가지 데이터 타입 컨테이너 포함

OrderedDict: 순서가 있는 사전형 자료형
OrderedDict객체.values(): 해당 사전의 value로 이루어진 리스트 반환
OrderedDict객체.reverse(): 순서 반전
