Chapter 3 신경망
입력 - 은닉 - 출력

은닉층?
    - 은닉층의 뉴런은 사람 눈에 보이지 않음

활성화 함수: h(x)
    - 입력신호 총합(a) -> 출력신호 변환하는 함수
=> 결과 노드를 확장 -> 내부에 (입력신호 -> 결과) 형태로 분리

종류: 계단함수, 시그모이드 함수
계단 함수
    - 임계값을 경계로 출력값(0 or 1) 변경

시그모이드 함수 (S자 모양 함수)
    - h(x)=1/(1+exp(-x))d

    계단함수 - 시그모이드함수 차이점
            => 그래프의 매끈함 -> 결과값의 차이(0 or 1 - 연속적인 실수)

    계단함수 - 시그모이드함수 공통점
            => 그래프 형태의 유사함(입력과 출력의 비례, 최소:0 - 최대: 1)
            => 비선형 함수

    신경망에서는 활성화 함수로 비선형 함수 사용
        -> 선형 함수는 층이 깊어도 '은닉층이 없는 네트워크'로 같은 기능 수행 가능

ReLU(Rectified Linear Unit) 함수
    - 입력 < 0 이면 0 출력, 입력 >= 0 이면 입력 출력

행렬의 내적: 행렬 곱

출력층 설계
문제: 분류, 회귀
    분류: 데이터가 어느 클래스에 속하는가? (ex 사진 속 인물 성별)
    회귀: 입력데이터에서 연속적인 수치 예측 (ex 사진 속 인물의 몸무게)

분류에서의 활성화 함수
    소프트맥스(softmax) 함수
    y=exp_a/sum_exp_a

    주의) a값이 클때 오버플로우 문제 발생!
        -> a에서 C'을 빼서 함수값으로 사용
        이때, 일반적으로 C'=입력값의 최대값

    특징
    1. 출력의 총합=1 -> 함수의 출력을 확률로 해석 가능
    2. 소프트맥스 함수는 단조 증가 함수 -> 신경망으로 '분류'를 할때는 무의미
            => 분류시에는 생략하는 것이 일반적

회귀에서의 활성화 함수
    항등함수: 입력 = 출력


numpy 관련
np.ndim(A) - 배열 A의 차원 수
A.shape - 원소 수
np.dot(A,B) - 행렬의 내적(곱) 계산


손글씨 숫자인식
MNIST 데이터셋 사용
PIL 오류 발생시 pillow 설치

전처리: 입력 데이터에 특정 변환을 가함
정규화: 데이터를 특정 범위로 변환하는 처리

