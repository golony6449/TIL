# Deep Learning From Scratch
---------------------------------
## Chapter 2. Perceptron (퍼셉트론)
* Model: 다수의 신호 -> (퍼셉트론) -> 하나의 신호
* AND, OR, NAND 구현 가능
* 가중치, 임계값으로 구정
* 가중치(w1, w2) - 노드(뉴런)에 곱해지는 임의의 값
* 임계값(theta) - 1을 출력하기 위한 입력 X 가중치 총합의 수준 
* 편향(bias) - theta를 -b로 치환해 가중치 총합 식으로 편입 (임계값에 -1 곱)

* 신경망: 퍼셉트론에서 가중치 매개변수(w1,w2)의 적절한 값을 데이터로부터 학습

## Chapter 3. Neural Network (신경망)
* 모델: 입력 - (은닉) - 출력

* 은닉층? - 은닉층의 뉴런은 사람 눈에 보이지 않음

### 활성화 함수
* h(x): 입력신호 총합(a) -> 출력신호 변환하는 함수
* => 결과 노드를 확장 -> 내부에 (입력신호 -> 결과) 형태로 분리

* 종류 - 계단함수, 시그모이드 함수, ReLU

#### 계단 함수
    - 임계값을 경계로 출력값(0 or 1) 변경

#### 시그모이드 함수 (S자 모양 함수)
    - h(x)=1/(1+exp(-x))d

    계단함수 - 시그모이드함수 차이점
            => 그래프의 매끈함 -> 결과값의 차이(0 or 1 - 연속적인 실수)

    계단함수 - 시그모이드함수 공통점
            => 그래프 형태의 유사함(입력과 출력의 비례, 최소:0 - 최대: 1)
            => 비선형 함수

    신경망에서는 활성화 함수로 비선형 함수 사용
        -> 선형 함수는 층이 깊어도 '은닉층이 없는 네트워크'로 같은 기능 수행 가능

#### ReLU(Rectified Linear Unit) 함수
    - 입력 < 0 이면 0 출력, 입력 >= 0 이면 입력 출력

* 행렬의 내적 = 행렬 곱

### 출력층 설계
* 문제: 분류, 회귀

#### 분류: 데이터가 어느 클래스에 속하는가? (ex 사진 속 인물 성별)
분류에서의 활성화 함수
    소프트맥스(softmax) 함수
    y=exp_a/sum_exp_a

    주의) a값이 클때 오버플로우 문제 발생!
        -> a에서 C'을 빼서 함수값으로 사용
        이때, 일반적으로 C'=입력값의 최대값

    특징
    1. 출력의 총합=1 -> 함수의 출력을 확률로 해석 가능
    2. 소프트맥스 함수는 단조 증가 함수 -> 신경망으로 '분류'를 할때는 무의미
            => 분류시에는 생략하는  것이 일반적

### Chapter 4. 신경망 학습
* 학습: 훈련 데이터로 부터 가중치 매개변수 최적값을 자동으로 찾는 것
* 학습목표: 손실 함수의 결과값을 가장 작게 만드는 매개변수 찾는 것을 목표

#### 손실 함수
* 신경망 학습에서 사용하는, 신경망의 <br>나쁨</br>을 나타내는 지표
* 종류: 일반적으로 평균제곱오차, 교차 엔트로피 오차 사용

##### 평균제곱오차
    E = (1/2)sum(yk-tk)^2
    y: 신경망의 출력 [0.1,0.05,0.6,0.0 ...]
    t: 정답레이블    [0,0,1,0,.....]
        -> 정답을 가리키는 위치: 1, 그 외: 0
    
    원-핫 인코딩: 정답레이블에서 한 원소만 1, 나머지는 0

##### 교차 엔트로피 오차
    E = -(sum(tk log yk) (log는 자연로그)
        주의) 구현할때, log()의 입력이 0이 되지 않도록 할 것
            (log0 = -무한대로 발산)

#### 미니배치 학습
    모든 훈련데이터에 대한 손실 함수의 합을 구하는 방법
    수백~수천만개의 훈련 데이터 중 학습을 위해 골라낸 데이터셋의 일부

손실함수를 설정하는 이유
    신경망 학습 - 손실 함수의 결과값을 가장 작게 만드는 매개변수를 찾음
        매개변수의 미분(기울기)을 계산 -> 매개변수 값을 서서히 갱신
        매개변수 변경시 함수 값이 연속적으로 변함

    궁극적인 목적: 정확도
    but, 정확도의 미분값은 대부분의 장소에서 0
        매개변수 변경시 변화 없거나, 불연속적으로 변화함
        -> 매개변수 갱신 불가능
        비슷한 이유로 계단함수 또한 손실함수로 사용하지 않음

    결론: 기울기가 0이 되지 않아야만, 신경망이 올바르게 학습할 수 있음

#### 미분
* 해석적미분 - 수식을 전개해 미분
        ex) y=x^2 -> y=2x

        오차가 없는 진정한 미분 값 구할 수 있음

* 수치미분 - 아주 작은 차분으로 미분

    미분: 접선의 기울기 -> x의 작은 변화가 f(x)를 얼마 만큼 변화시키는가...
    

    구현: h=10e-50 
        문제사항
            1. 반올림 오차 발생 
                -> h=10e-4로 변경
            2. h를 무한히 0 으로 만들 수 없음 -> 오차 발생
                -> (x-h), (x+h)일때의 차분을 계산하는 방법으로 오차 줄임
                (x를 중심으로 전후의 차분 계산: 중심차분(=중앙차분))

#### 편미분
* 변수가 여럿인 함수에 대한 미분

    구현 1.
        1. x0, x1중 하나를 고정
        2. 나머지 변수로만 이루어진 함수 생성
        3. 수치미분
    
    구현 2.
        기울기 계산

#### 기울기
    - 모든 변수의 편미분을 벡터로 정리한 것
    기울기가 가리키는 방향: 각 장소에서 함수의 출력값을 가장 크게 줄이는 방향

    경사법
        - 기울기를 이용해 함수의 최솟값을 찾는 방법
        기울어진 방향으로 일정 거리 이동 -> 해당 지점에서 기울기 계산 -> 이동 -> ...

        최솟값을 찾는 경사법 - 경사하강법
        최댓값을 찾는 경사법 - 경사상승법

        주의) 함수값을 낮추는 방안을 제시하는 지표: 기울기
            but, 정말 최솟값이 있는지, 방향이 맞는지 보장 불가능함
            하지만 해당 방향으로 이동시, 함수의 값을 줄일수는 있음
                => 최솟값의 장소를 찾는 문제에서 나아갈 방향의 단서로 사용

            극소값, 극댓값, 안장점에서 기울기 = 0
                    -> 기울기=0 이 최소값이라고 보장 할 수 없음
        
        학습률
            - 한 번의 학습으로 얼마만큼 학습할지(매개변수 값을 갱신 할지) 결정하는 상수
            너무 크거나 작으면 '좋은 장소'를 찾을 수 없음
        
        신경망에서의 기울기
            - 가중치 매개변수에 대한 손실함수의 기울기
            L: 손실함수, W: 가중치 일때
                편미분의 형상이 W와 같음

    하이퍼 파라메터 - 사람이 직접 설정해야 하는 매개변수 (ex: 학습률)

학습 알고리즘 구현하기

    확률적 경사 하강법(Stochastic gradient descent)
        - 확률적으로 무작위로 골라낸 데이터에 대해 수행하는 경사 하강법
    전제: 적응가능한 가중치, 편향 존재
            학습: 파라메터(가중치, 편향)를 훈렬 데이터에 적응하도록 조정하는 과정
    1. - 미니배치
        훈련 데이터 중 일부를 무작위로 가져옴 <= 미니배치
        목표: 미니배치의 손실 함수값 최소화
    2. - 기울기 산출
        목표달성을 위해 각 가중치 매개변수의 기울기 구함
        이때, 기울기는 목표 달성을 위한 방향 제시
    3. - 매개변수 갱신
        가중치 매개변수를 기울기 방향으로 아주 조금 갱신
    4. - 반복

시험 데이터로 평가하기
    에폭(epoch) - 학습에서 훈련데이터를 모두 소진했을때의 횟수를 나타내는 단위 (= 전체 데이터를 배치화 했을때의 배치 수)
    ex) 훈련데이터 10000개를 100개의 미니배치로 학습 할 경우 -> 100회 반복
            -> 1에폭 = 100회

#### 회귀: 입력데이터에서 연속적인 수치 예측 (ex 사진 속 인물의 몸무게)
회귀에서의 활성화 함수
    항등함수: 입력 = 출력

손글씨 숫자인식
MNIST 데이터셋 사용

전처리: 입력 데이터에 특정 변환을 가함
정규화: 데이터를 특정 범위로 변환하는 처리

신경망 학습에서 가중치 매개변수의 기울기를 수치 미분으로 구했음
    문제점: 수치미분의 계산시간
            -> 오차역계산법 등장

### Chapter 5 오차역전파법
* 오차를 역(반대방향)으로 전파하는 방법

    계산그래프
        1. 그래프 생성
        2. 계산진행 - 왼쪽 -> 오른쪽
                        => 순전파(출발점 -> 종착점 으로의 전파)
        
        특징
            국소적 계산: 자신과 관련된 계산 외에는 관여 X
    
        이점
            국소적 계산 - 전체가 복잡하더라도, 각 노드의 단순한 계산으로 문제 해결 가능
            중간 계산 결과 보관
            <b> 역전파(종착점 -> 출발점)를 통해 미분을 효율적으로 계산 가능 </b>

    연쇄법칙
        다변수의 함성함수에 대한 편미분
        `만화로 쉽게 배우는 미분적분` 참고

    역전파
        - 계산 그래프의 오른쪽에서 왼쪽으로 신호 전파
        - 노드로 들어온 이력신호에 해당 노드의 편미분을 곱한 후 다음 노드로 전달
        - 연쇄법칙과 동일한 원리
    
    덧셈노드의 역전파
        z=x+y 일때
        ∂z/∂x = 1, ∂z/∂y = 1
        => 입력값 그대로 출력

    곱셈노드의 역전파
        z=xy 일때
        ∂z/∂x = y, ∂z/∂y = x
        => 서로의 입력신호를 바꾼 값을 곱해서 출력
        사전에 입력신호를 저장해야 할 필요 있음

    계층 구현
      계층 - 신경망의 기능단위
      계층은 class로 구현하고, forward(), backward()를 메소드로 가지도록 구현

      활성화 함수 계층 - ReLU, Sigmoid
        ReLU
            y=x(x>0) or 0 (x<=0)
            -> dy/dx=1(x>0) or 0(x<=0)
            
            순전파 시점에서 값이 통과 했으면 역전파에서 값 통과, else 통과 X
        
        Sigmoid
            exp() 노드, / 노드 2개로 구성
            X -> exp -> + -> / ->

        Affine 계층
            행렬의 내적을 구하는 계층
                
                전치행렬 - (i,j)위치 -> (j,i)위치로 변경

        배치용 Affine 계층
            주의) 편향은 X*W 의 각 데이터에 더해짐 -> 역전파 할때는 역전파 값이 편향의 원소로 모여야 함

        Softmax with loss 계층
            Softmax 계층 + Cross Entropy Error 계층
            역전파 결과 (y1-t1,y2-t2,y3-t3) (=출력, 정답레이블의 차분)


    오차역전파법 구현
        학습: 미니배치 생성 -> 기울기 산출 -> 매개변수 수정
                        <-              <-          <-
0704 부터

### numpy
* 수치계산용 라이브러리 - 수학알고리즘, 배열조작 메소드 포함

* numpy.array() - 넘파이 배열 생성 (인수: 리스트)
* 배열-배열, 배열-스칼라값(수치) 연산 가능 (단, 배열의 경우는 n(원소) 같을 때!)
* 배열-스칼라값 연산: 브로드캐스트 (1개의 값을 모든 원소에 전달)
* 비교연산 가능

* 객체.flatten() - 다차원 배열 -> 1차원 배열
* numpy.arrange(0,6,0.1) - 0 -> 6까지 0.1 간격의 배열 생성
* numpy.sin(배열) - 원소의 sin값 계산

* np.ndim(A) - 배열 A의 차원 수
* A.shape - 원소 수
* np.dot(A,B) - 행렬의 내적(곱) 계산

* np.random.choice(60000,10) 0~59999(60000 미만)의 숫자 중 10개 추출

* array객체.astype(np.자료형) - 객체 원소를 해당 자료형으로 변경
* np.zeros_like(x) - 배열 x와 형상이 같고, 원소는 0인 배열 반환

| 종류 | 명칭 |
|-------|-------|
| 1차원 배열 | 벡터(vector) |
| 2차원 배열 | 행렬(matrix) |
| 벡터, 행렬 일반화 | 텐서(Tensor) |

### matplotlib
* 그래프 라이브러리
* 결과, 중간데이터 -> 시각화

* 주의: 에러 발생시 font_manager 수정
* `direc = os.path.abspath(direc).lower()` -> `direc = direc.split('\0', 1)[0]` 로 변경

* PIL 오류 발생시 pillow 설치

* 파일경로
* C:\Users\bb016\AppData\Local\Programs\Python\Python35\Lib\site-packages\matplotlib
* 참고자료: Error in import matplotlib.pyplot (Windows 10 64bit) (https://stackoverflow.com/q/34004063/395857)

* Example
```python
import matplotlib.pyplot as plt
plt.plot(x축, y축) #원소가 각각의 x.y 좌표를 가지는 그래프 생성
plt.show() #그래프 출력
```