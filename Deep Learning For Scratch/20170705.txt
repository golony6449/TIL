Chapter 6 학습 관련 기술 ... 이어서

가중치의 초기값

가중치 감소 기법
1. 오버피팅을 억제해 범용 성능 향상 기법
2. 가중치 매개변수의 값이 작아지도록 학습

.1. 가중치가 0이면?
올바른 학습 불가능
이유: 오차역전파법에서는 모든 가중치 값이 똑같이 갱신
-> 가중치를 여러개 가지는 의미를 사라지게 함
이와 같이 가중치가 고르게 되는 상황을 막기 위해 초기값은 무작위로 설정해야 함

.2. 은닉층의 활성화값 분포
활성화값: 활성화 함수의 출력데이터

가중치의 초기값: 1 일때
활성화 값이 0,1에 치우쳐 분포 -> 역전파의 기울기값이 점점 작아지다가 사라짐
기울기 소실 문제 발생!

가중치의 초기값: 0.001 일때
0.5 부근에 활성화 값 집중
다수의 뉴런이 같은 값 가짐 -> 뉴런을 여러개 둔 의미 없음
표현력 제한 문제 발생!

가중치의 초기값: Xavier 초깃값
앞계층의 노드: n개 -> 가중치: 1/root(n)
활성화 함수가 선형인 경우에 적합

.3. ReLU 사용시의 가중치 초기값
ReLU는 선형함수가 아니기 때문에 Xavier 초기값 사용 힘듦
ReLU 특화 초기값인 He 초기값 사용
He 초기값: root(2/n)

배치 정규화
각 층이 활성화 값을 적당히 퍼뜨리도록 강제하는 것

배치 정규화 알고리즘
장점
1. 학습을 빨리 진행할 수 있음
2. 초깃값에 크게 의존하지 않음
3. 오버피팅 억제

적용 방법
Affine, ReLU 계층 사이에 배치정규화계층 삽입
역할
입력데이터를 평균 0, 분산이 1이 되도록 정규화 및 각 게층 별로 확대, 이동 변환 수행

오버피팅
신경망이 훈련데이터에 너무 적응 -> 그 이외의 데이터에는 대응 하지 못하는 상태
원인
1. 매개변수가 많고, 표현력이 높은 모델
2. 훈련 모델의 부족

오버피팅 억제방법
1. 가중치 감소
2. 드롭아웃

가중치 감소
가중치에 비례해 큰 패널티 부과
(오버피팅은 대개 가중치 값이 커서 발생)

방법
손실함수에 가중치의 제곱노릅(L2 노름)을 더함
-> L2 노름에 따른 가중치 감소량: (1/2)*lambda*(W**2)

효과
훈련데이터 - 시험데이터 정확도간의 차이 감소

드롭아웃
신경망 모델이 복잡해 졌을때, 가중치 감소만으로는 대응하기 어렵다는 문제점 해결을 위해 도입
훈련시에 은닉층의 뉴런을 무작위로 골라 삭제 -> 삭제된 뉴런에는 신호 전달 X
시험시에는 모든 뉴런에 신호 전달
(단, 시험때 삭제된 뉴런은 삭제한 비율을 곱해서 출력)

삭제되지 않은 뉴런은 비율 곱 없이 출력

핵심
순전파시에 mask에 삭제할 뉴런을 표시!
삭제된 뉴런은 순전파, 역전파 모두 신호 차단

적절한 하이퍼파라미터 값 찾기
복습: 하이퍼파라미터 - 사람이 직접 정해주어야 하는 수치 (뉴런수, 배치크기, 학습률, 가중치감소)

방법 1. 검증데이터 이용
학습: 학습데이터, 범용성능평가: 시험데이터
이때, 하이퍼파라미터 성능 평가시에는 시험데이터를 사용하면 안됨
-> 파라미터 값이 시험데이터에 오버피팅

검증 데이터: 하이퍼파라미터 전용 확인 데이터, 파라미터의 성능(적절성) 평가
간단한 생성 방법: 훈련 데이터 중 20% 정도를 검증데이터로 분리

하이퍼파라미터 최적화
최적값이 존재하는 범위를 조금씩 줄여나감
절차
1. 대략적인 범위 설정
2. 범위 내에서 파라미터 값을 무작위 샘플링
3. 샘플링한 값을 사용해 학습후 검증데이터로 평가
4. 2,3 단계를 일정횟수 반복해 결과를 보고 범위 축소

최적화에는 장시간 소요 ->
학습을 위한 에폭을 작게해 1회 평가에 걸리는 시간을 단축하는 것이 효과적

Chapter 7 합성곱 신경망(CNN) - Convolutional Neural Network
여러분야(이미지, 음석 인식)에서 사용 됨.
특히 이미지 인식분야에서의 딥러닝 기법은 대부분 CNN을 기반

